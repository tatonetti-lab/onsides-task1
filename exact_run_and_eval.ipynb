{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "706f8453-d366-4e8c-976c-90b59cf58197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "from common_string import common_lenient_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db80a89-6744-4602-936e-c5457d547d20",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8759fb31-5cee-46e8-a65f-36bf14b08730",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd0bffb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_subtype(manual_ades, gpt_output, drug, section='adverse reactions', subtype = 'all', lenient=False):\n",
    "    '''\n",
    "    For a given drug, evaluate the performance of GPT on a given subtype of ADEs. \n",
    "    '''\n",
    "\n",
    "    if not section in ('adverse reactions', 'boxed warnings', 'warnings and precautions'):\n",
    "        raise Exception(f\"Unexpected section, {section}, provided.\")\n",
    "    \n",
    "    drug_df = manual_ades.query(\"(drug_name == '{}') & (section_name == '{}')\".format(drug, section))\n",
    "    if subtype == 'all':\n",
    "        pass\n",
    "    elif subtype == 'exact-meddra': \n",
    "        drug_df = drug_df[drug_df.meddra_exact_term == 1]\n",
    "    elif subtype == 'non-meddra': \n",
    "        drug_df = drug_df[drug_df.meddra_exact_term == 0]\n",
    "    elif subtype == 'negated': \n",
    "        drug_df = drug_df[drug_df.negated_term == 1]\n",
    "    elif subtype == 'discontinuous': \n",
    "        drug_df = drug_df[drug_df.discontinuous_term == 1]\n",
    "    else:\n",
    "        raise Exception(f\"Unexpected subtype, {subtype}, provided.\")\n",
    "\n",
    "    \n",
    "    manual = set(drug_df['reaction_string'].to_list())\n",
    "    gpt_drug = (gpt_output[\n",
    "        (gpt_output['drug_name'] == drug)\n",
    "        &\n",
    "        (gpt_output['section_name'] == section)\n",
    "        ][\"gpt_output\"].astype(str)\n",
    "        .str.lower()\n",
    "        .str.replace('\\n-', ', ')\n",
    "        .str.split(\",\").tolist())\n",
    "    \n",
    "    try:\n",
    "        gpt_drug = [x.strip() for x in gpt_drug[0] if x]\n",
    "        gpt_drug = set(gpt_drug)\n",
    "    except:\n",
    "        return [drug, section, subtype, len(manual), len(gpt_drug), np.nan, np.nan, np.nan, np.nan, np.nan, np.nan]\n",
    "        \n",
    "    if not lenient:    \n",
    "        #overall\n",
    "        TP = len(manual.intersection(gpt_drug))\n",
    "        FP = len(gpt_drug.difference(manual))\n",
    "        FN = len(manual.difference(gpt_drug))\n",
    "        if TP == 0 and FP == 0:\n",
    "            precision = np.NAN\n",
    "        else:\n",
    "            precision = TP/(TP+FP)\n",
    "        if TP == 0 and FN == 0:\n",
    "            recall = np.NAN\n",
    "        else:\n",
    "            recall = TP/(TP+FN)\n",
    "        if precision != 0 and recall != 0:\n",
    "            f1 = (2 * precision * recall)/(precision + recall)# 2*TP/(2*TP+FP+FN)\n",
    "        else:\n",
    "            f1 = np.NAN\n",
    "    else:\n",
    "        [TP, FP, FN, precision, recall, f1] = common_lenient_performance(gpt_drug, manual)\n",
    "    \n",
    "    if subtype != 'all':\n",
    "            # these can't be computed for the subtypes\n",
    "            precision = np.nan\n",
    "            f1 = np.nan\n",
    "            FP = np.nan\n",
    "    \n",
    "    return [drug, section, subtype, len(manual), len(gpt_drug), TP, FP, FN, precision, recall, f1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8c6cc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(manual_ades, gpt_output, lenient=False, limit = 1000):\n",
    "    drugs = gpt_output['drug_name'].unique()\n",
    "    drugs_set = set()\n",
    "    results = []\n",
    "    for drug in tqdm(drugs):\n",
    "            results.append(evaluation_subtype(manual_ades, gpt_output, drug, lenient))        \n",
    "    results = pd.DataFrame(results, columns=['drug_name', 'exclude', 'n_manual', 'n_gpt', 'tp', 'fp', 'fn', 'precision', 'recall', 'f1'])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d669d0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_granular(manual_ades, gpt_output, limit = 1000, lenient=False):\n",
    "    drugs = gpt_output['drug_name'].unique()\n",
    "    drugs_set = set()\n",
    "    results = []\n",
    "    for drug in tqdm(drugs):\n",
    "        drugs_set.add(drug)\n",
    "        if len(drugs_set) > limit:\n",
    "            break\n",
    "        \n",
    "        for section in ['adverse reactions', 'warnings and precautions','boxed warnings']:\n",
    "            for subtype in ['all', 'exact-meddra', 'non-meddra', 'negated', 'discontinuous']:\n",
    "                results.append(evaluation_subtype(manual_ades, gpt_output, drug, section, subtype, lenient))\n",
    "\n",
    "    results = pd.DataFrame(results, columns=['drug_name', 'section', 'ade_type', 'n_manual', 'n_gpt', 'tp', 'fp', 'fn', 'precision', 'recall', 'f1'])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37930311-07c7-4762-abd1-cb249c5bd25d",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed8cc025-3af6-4bea-826b-962eb8b36f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_file = 'data/train_drug_label_text.csv'\n",
    "manual_file = 'data/train_drug_label_text_manual_ades.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22518e4d-ec19-4a56-9914-cd969a50cbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "drugs = pd.read_csv(drug_file)\n",
    "manual_ades = pd.read_csv(manual_file)\n",
    "set_type = drug_file.split('/')[1].split('_')[0] # assuming file follows format \"train_...\" or \"test....\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82ae7da-2a0e-4755-ae4d-4bced5f7f5a2",
   "metadata": {},
   "source": [
    "## Run GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e618b1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79125bf3",
   "metadata": {},
   "source": [
    "## Exact Match Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "704c226e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 239/239 [01:25<00:00,  2.79it/s]\n"
     ]
    }
   ],
   "source": [
    "run_key = 'exact_{}'.format(set_type)\n",
    "\n",
    "if not os.path.exists('results/{}.csv'.format(run_key)):\n",
    "    # load the meddra terms\n",
    "    fh = open('data/meddra_llt_pt_map.txt')\n",
    "    reader = csv.reader(fh, delimiter='|')\n",
    "    header = next(reader)\n",
    "\n",
    "    meddra_llt_terms = set()\n",
    "    meddra_pt_terms = set()\n",
    "\n",
    "    for row in reader:\n",
    "        meddra_llt_terms.add(row[1].lower())\n",
    "        meddra_pt_terms.add(row[4].lower())\n",
    "    \n",
    "    fh.close()\n",
    "\n",
    "    meddra_terms = meddra_llt_terms | meddra_pt_terms\n",
    "    len(meddra_llt_terms), len(meddra_pt_terms), len(meddra_terms)\n",
    "\n",
    "    results = list()\n",
    "    for _, row in tqdm(drugs.iterrows(), total=drugs.shape[0]):\n",
    "        name, section = row['drug_name'], row['section_name']\n",
    "        text = row['section_text'].lower()\n",
    "        \n",
    "        found_terms = set()\n",
    "        for term in meddra_terms:\n",
    "            if text.find(term) != -1:\n",
    "                found_terms.add(term)\n",
    "        \n",
    "        exact_out = ', '.join(list(found_terms))\n",
    "        \n",
    "        results.append([name, section, exact_out])\n",
    "\n",
    "    exact_output = pd.DataFrame(\n",
    "        [r for r in results if r is not None],\n",
    "        columns=['drug_name', 'section_name', 'gpt_output']\n",
    "    )\n",
    "    exact_output.to_csv('results/{}.csv'.format(run_key))\n",
    "    \n",
    "    outputs[run_key] = exact_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7986085b",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3e1252e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exact_train\n"
     ]
    }
   ],
   "source": [
    "for run_key in sorted(outputs.keys()):\n",
    "    print(run_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "787d7804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exact_train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:02<00:00, 49.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exact_train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:22<00:00,  4.44it/s]\n"
     ]
    }
   ],
   "source": [
    "for eval_method in ('strict', 'lenient'):\n",
    "    for run_key, output in outputs.items():\n",
    "        granular_save_filename = 'results/{}_{}_granular.csv'.format(run_key, eval_method)\n",
    "        overall_save_filename = 'results/{}_{}_overall.csv'.format(run_key, eval_method)\n",
    "        \n",
    "        print(run_key)\n",
    "        \n",
    "        results_granular = evaluation_granular(manual_ades, output, lenient=(eval_method=='lenient'))\n",
    "        overall_results = results_granular.groupby(['section','ade_type'])[['tp', 'fp', 'fn']].sum(min_count = 1).reset_index()\n",
    "        overall_results['micro_precision'] = overall_results['tp']/(overall_results['tp']+overall_results['fp'])\n",
    "        overall_results['micro_recall'] = overall_results['tp']/(overall_results['tp']+overall_results['fn'])\n",
    "        overall_results['micro_f1'] = (2 * overall_results['micro_precision'] * overall_results['micro_recall'])/(overall_results['micro_precision'] + overall_results['micro_recall']) # 2*tp_total/(2*tp_total+fp_total+fn_total)\n",
    "        \n",
    "        macro_results = results_granular.groupby(['section', 'ade_type'])[['precision', 'recall', 'f1']].mean(numeric_only=True).reset_index()\n",
    "        overall_results['macro_precision'] = macro_results['precision']\n",
    "        overall_results['macro_recall'] = macro_results['recall']\n",
    "        overall_results['macro_f1'] = macro_results['f1']\n",
    "\n",
    "        allsections_results = results_granular.groupby(['ade_type'])[['tp', 'fp', 'fn']].sum(min_count = 1).reset_index().query(\"ade_type == 'all'\")\n",
    "        allsections_results['micro_precision'] = allsections_results['tp']/(allsections_results['tp']+allsections_results['fp'])\n",
    "        allsections_results['micro_recall'] = allsections_results['tp']/(allsections_results['tp']+allsections_results['fn'])\n",
    "        allsections_results['micro_f1'] = (2 * allsections_results['micro_precision'] * allsections_results['micro_recall'])/(allsections_results['micro_precision'] + overall_results['micro_recall']) # 2*tp_total/(2*tp_total+fp_total+fn_total)\n",
    "        \n",
    "        allsections_macro_results = results_granular.groupby(['ade_type'])[['precision', 'recall', 'f1']].mean(numeric_only=True).reset_index().query(\"ade_type == 'all'\")\n",
    "        allsections_results['macro_precision'] = allsections_macro_results['precision']\n",
    "        allsections_results['macro_recall'] = allsections_macro_results['recall']\n",
    "        allsections_results['macro_f1'] = allsections_macro_results['f1']\n",
    "        allsections_results['section'] = ['all']\n",
    "        \n",
    "        overall_results = pd.concat([overall_results, allsections_results])\n",
    "        overall_results.to_csv(overall_save_filename)\n",
    "        results_granular.to_csv(granular_save_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d376cd5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
