{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "706f8453-d366-4e8c-976c-90b59cf58197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "from evaluation_functions import evaluate\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db80a89-6744-4602-936e-c5457d547d20",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8759fb31-5cee-46e8-a65f-36bf14b08730",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ed0377f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_extraction(model, prompt, text, temperature, max_length):\n",
    "    # ssh -N -f -L localhost:5000:localhost:5000 username@10.19.2.120\n",
    "    llamasgard_endpoint = \"http://localhost:5000/predict\"\n",
    "\n",
    "    # model = codellama/CodeLlama-34b-Instruct-hf\n",
    "    # Define the payload\n",
    "    payload = {\n",
    "        \"input\": prompt.format(text),\n",
    "        \"model_id\": model,\n",
    "        \"parameters\": {\n",
    "            \"temperature\": temperature,\n",
    "            \"max_length\": max_length\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    #print(prompt.format(text))\n",
    "\n",
    "    response = requests.post(llamasgard_endpoint, json=payload)\n",
    "\n",
    "    # Check the response status code\n",
    "    if response.status_code != 200:\n",
    "        print(\"Error:\", response.json())\n",
    "\n",
    "    #print(\"\\n\\n\" + str(response.json()) + \"\\n\")\n",
    "    return response.json().get('response')\n",
    "\n",
    "def perform_cleanup(extraction, openai_api):\n",
    "    client = OpenAI(api_key=openai_api)\n",
    "    \n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"\"},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"\"\"The following text is an extraction of adverse event terms from a drug label. Please remove any preamble or postamble from the list and turn the list of ADEs into a comma separated list. \n",
    "The text: {}\"\"\".format(extraction)\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-3.5-turbo-16k\",\n",
    "        temperature=0,\n",
    "    )\n",
    "    term = chat_completion.choices[0].message.content\n",
    "    return term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57ec29cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# row['section_text']\n",
    "# perform_extraction(\"google/gemma-7b\", system_content, prompt, row['section_text'], 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98fa878b-b40a-4895-808e-9574b7d004c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for extracting \n",
    "def extract_ade_terms(config, model, prompt, text, temperature, max_length):\n",
    "  extraction = perform_extraction(model, prompt, text, temperature, max_length)\n",
    "  if extraction is None:\n",
    "    raise Exception(f\"perform_extraction() return None for {model}\")\n",
    "  else:\n",
    "    extraction = perform_cleanup(extraction, config['OpenAI']['openai_api_key'])\n",
    "    return extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37930311-07c7-4762-abd1-cb249c5bd25d",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed8cc025-3af6-4bea-826b-962eb8b36f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_file = 'data/TAC2017/train_drug_label_text.csv'\n",
    "manual_file = 'data/TAC2017/train_drug_label_text_manual_ades.csv'\n",
    "my_max = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22518e4d-ec19-4a56-9914-cd969a50cbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "drugs = pd.read_csv(drug_file)\n",
    "manual_ades = pd.read_csv(manual_file)\n",
    "set_type = drug_file.split('/')[2].split('_')[0] # assuming file follows format \"train_...\" or \"test....\"\n",
    "\n",
    "all_sections = drugs.query(\"section_name != 'all-concat'\").groupby('drug_name')['section_text'].apply(' '.join).reset_index()\n",
    "all_sections.insert(1, \"section_name\", [\"all-concat\" for _ in range(all_sections.shape[0])])\n",
    "drugs = pd.concat([drugs, all_sections])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82ae7da-2a0e-4755-ae4d-4bced5f7f5a2",
   "metadata": {},
   "source": [
    "## Run Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e618b1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6f04e8a-be72-4ad8-abfe-7b5d60c03a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = json.load(open('./config.json'))\n",
    "\n",
    "# gpt_model = 'code-llama-34b'\n",
    "# model_id = \"codellama/CodeLlama-34b-Instruct-hf\"\n",
    "\n",
    "# model_id = \"google/gemma-7b\"\n",
    "# model_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "model_name = model_id.split('/')[1]\n",
    "\n",
    "max_length = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4b1bbd7-3b6c-42f5-a2af-04391ea86537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Meta-Llama-3-8B-Instruct_fatal-prompt-v2_pharmexpert-v1_temp0_train'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nruns = 1\n",
    "temperature = 0\n",
    "\n",
    "system_options = {\n",
    "    \"no-system-prompt\": \"\",\n",
    "    \"pharmexpert-v0\": \"You are an expert in pharmacology.\",\n",
    "    \"pharmexpert-v1\": \"You are an expert in medical natural language processing, adverse drug reactions, pharmacology, and clinical trials.\"\n",
    "}\n",
    "\n",
    "prompt_options = {\n",
    "    \"fatal-prompt-v2\": \"\"\"\n",
    "Extract all adverse reactions as they appear, including all synonyms.\n",
    "mentioned in the text and provide them as a comma-separated list.\n",
    "If a fatal event is listed add 'death' to the list.\n",
    "The text is :'{}' \n",
    "\"\"\",\n",
    "    \"gpt-written-prompt-llama\": \"\"\"\n",
    "Extract adverse drug reaction information comprehensively from a given drug label text. Ensure \n",
    "inclusivity of sentence-form expressions, reactions in tables, negated reactions, discontinuous \n",
    "mentions, hypothetical scenarios, and potentially fatal occurrences. Output the exact mentions \n",
    "of adverse reactions in a comma-separated format.\n",
    "The text is :'{}'\n",
    "\"\"\",\n",
    "    \"gpt-written-prompt-mixtral\": \"\"\"\n",
    "Please extract adverse drug reactions (ADRs) from the provided structured product label (SPL) information. The SPL contains detailed data on potential reactions to a specific medication. Your task is to identify adverse reactions, including negated, discontinuous, hypothetical, and potentially fatal reactions. The adverse reactions can be presented in sentence form or may appear within tables.\n",
    "\n",
    "Instructions for Mixtral:\n",
    "\n",
    "1. Analyze the provided structured product label data thoroughly.\n",
    "2. Identify adverse reactions associated with the medication mentioned in the SPL.\n",
    "3. Ensure the extraction covers various scenarios:\n",
    " - Negated reactions (e.g., \"no adverse reactions\")\n",
    " - Discontinuous reactions (e.g., \"dizziness, nausea, and vomiting\")\n",
    " - Hypothetical reactions (e.g., \"may cause headache\")\n",
    " - Potentially fatal reactions (e.g., \"risk of cardiac arrest\")\n",
    "4. Present the extracted adverse reactions as a comma-separated list of terms.\n",
    "The text is :'{}'\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "system_name = \"pharmexpert-v1\"\n",
    "system_content = system_options[system_name]\n",
    "\n",
    "user_prompt_name = \"fatal-prompt-v2\"\n",
    "user_prompt = prompt_options[user_prompt_name]\n",
    "\n",
    "gpt_params = [f\"temp{temperature}\"]\n",
    "\n",
    "if model_id.split('/')[0] in (\"codellama\", \"mistralai\"):\n",
    "    print(\"Modifying the prompt to include instruction tags.\")\n",
    "    prefix = \"\"\n",
    "    prompt = f\"<s>[INST] <<SYS>>\\\\n{system_content}\\\\n<</SYS>>\\\\n\\\\n{user_prompt}[/INST]{prefix}\"\n",
    "else:\n",
    "    prompt = system_content + '\\n' + user_prompt\n",
    "\n",
    "output_file_basename = '{}_{}_{}_{}_{}'.format(model_name, user_prompt_name, system_name, '-'.join(gpt_params), set_type)\n",
    "output_file_basename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae503391",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tries = 2\n",
    "num_tries = defaultdict(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "492d7e82-894f-4c0f-bbf1-4de819b94b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3-8B-Instruct_fatal-prompt-v2_pharmexpert-v1_temp0_train_run0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/340 [00:00<00:12, 27.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping run ('Meta-Llama-3-8B-Instruct_fatal-prompt-v2_pharmexpert-v1_temp0_train_run0', 0) because we have tried it 2 times.\n",
      "Error: {'error': '`temperature` (=0) has to be a strictly positive float, otherwise your next token scores will be invalid.'}\n",
      "Encountered an exception for row: 'XEOMIN' 'boxed warnings'.\n",
      "This is the 1 time we have tried to run this. Will try 2 times and then skip.\n",
      "Will save progress, so you can restart from where we left off.\n",
      "Failed for prompt: You are an expert in medical natural language processing, adverse drug reactions, pharmacology, and clinical trials.\n",
      "\n",
      "Extract all adverse reactions as they appear, including all synonyms.\n",
      "mentioned in the text and provide them as a comma-separated list.\n",
      "If a fatal event is listed add 'death' to the list.\n",
      "The text is :'\n",
      "\n",
      "    BOXED WARNING: WARNING: DISTANT SPREAD OF TOXIN EFFECT\n",
      "\n",
      "    WARNING: DISTANT SPREAD OF TOXIN EFFECT  \n",
      "\n",
      "    Postmarketing reports indicate that the effects of XEOMIN and all botulinum toxin products may spread from the area of injection to produce symptoms consistent with botulinum toxin effects. These may include asthenia, generalized muscle weakness, diplopia, blurred vision, ptosis, dysphagia, dysphonia, dysarthria, urinary incontinence and breathing difficulties. These symptoms have been reported hours to weeks after injection. Swallowing and breathing difficulties can be life threatening and there have been reports of death. The risk of symptoms is probably greatest in children treated for spasticity but symptoms can also occur in adults treated for spasticity and other conditions, particularly in those patients who have underlying conditions that would predispose them to these symptoms. In unapproved uses, including spasticity in children and adults, and in approved indications, cases of spread of effect have been reported at doses comparable to those used to treat cervical dystonia and at lower doses     [see   Warnings and Precautions (5.1)  ]    .  \n",
      "\n",
      "\n",
      "\n",
      "   EXCERPT:     WARNING: DISTANT SPREAD OF TOXIN EFFECT  \n",
      "\n",
      "\n",
      "\n",
      " See full prescribing information for complete   boxed warning  . The effects of   XEOMIN   and all botulinum toxin products may spread from the area of injection to produce symptoms consistent with botulinum toxin effects. These symptoms have been reported hours to weeks after injection. Swallowing and breathing difficulties can be life threatening and there have been reports of death. The risk of symptoms is probably greatest in children treated for spasticity but symptoms can also occur in adults, particularly in those patients who have underlying conditions that would predispose them to these symptoms. (  5.1  )\n",
      "' \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "perform_extraction() return None for meta-llama/Meta-Llama-3-8B-Instruct",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 53\u001b[0m\n\u001b[1;32m     50\u001b[0m             gpt_output\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/extract/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(run_key))\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed for prompt: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;241m.\u001b[39mformat(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 53\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     56\u001b[0m gpt_output \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[1;32m     57\u001b[0m     [r \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results \u001b[38;5;28;01mif\u001b[39;00m r \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m],\n\u001b[1;32m     58\u001b[0m     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdrug_name\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msection_name\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt_output\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     59\u001b[0m )\n",
      "Cell \u001b[0;32mIn[14], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m     text \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msection_text\u001b[39m\u001b[38;5;124m'\u001b[39m][:\u001b[38;5;241m14000\u001b[39m]\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 36\u001b[0m     gpt_out \u001b[38;5;241m=\u001b[39m \u001b[43mextract_ade_terms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend([name, section, gpt_out])    \n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m, in \u001b[0;36mextract_ade_terms\u001b[0;34m(config, model, prompt, text, temperature, max_length)\u001b[0m\n\u001b[1;32m      3\u001b[0m extraction \u001b[38;5;241m=\u001b[39m perform_extraction(model, prompt, text, temperature, max_length)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extraction \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m----> 5\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperform_extraction() return None for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m   extraction \u001b[38;5;241m=\u001b[39m perform_cleanup(extraction, config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOpenAI\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenai_api_key\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mException\u001b[0m: perform_extraction() return None for meta-llama/Meta-Llama-3-8B-Instruct"
     ]
    }
   ],
   "source": [
    "# run local model\n",
    "for i in range(nruns):\n",
    "    run_key = \"{}_run{}\".format(output_file_basename, i)\n",
    "    print(run_key)\n",
    "    \n",
    "    if run_key in outputs:\n",
    "        print(f\"Run {run_key} already started will pick up from where it was left off.\")\n",
    "    elif os.path.exists('results/extract/{}.csv'.format(run_key)):\n",
    "        gpt_output = pd.read_csv('results/extract/{}.csv'.format(run_key))\n",
    "        outputs[run_key] = gpt_output\n",
    "        print(f\"Run {run_key} started, loading from disk and pick up from where it was left off.\")\n",
    "    \n",
    "    start = time.time()\n",
    "    results = list()\n",
    "    for i, row in tqdm(drugs.iterrows(), total=drugs.shape[0]):\n",
    "\n",
    "        if num_tries[(run_key,i)] >= max_tries:\n",
    "            print(f\"Skipping run {(run_key,i)} because we have tried it {max_tries} times.\")\n",
    "            continue\n",
    "        \n",
    "        name, section = row['drug_name'], row['section_name']\n",
    "\n",
    "        if run_key in outputs:\n",
    "            prev_run_results = outputs[run_key].query(f\"drug_name == '{name}'\").query(f\"section_name == '{section}'\")\n",
    "            if prev_run_results.shape[0]==1:\n",
    "                results.append([name, section, prev_run_results.gpt_output.values[0]])\n",
    "                continue\n",
    "        \n",
    "        text = row['section_text'][:15000]\n",
    "        \n",
    "        if (name in ('PROLIA', 'ELIQUIS', 'INVOKANA') and section == 'adverse reactions') \\\n",
    "            or (name in ('PROLIA', 'ELIQUIS','INVOKANA') and section == 'all-concat'):\n",
    "            text = row['section_text'][:14000]\n",
    "        \n",
    "        try:\n",
    "            gpt_out = extract_ade_terms(config, model_id, prompt, text, temperature, max_length)\n",
    "            results.append([name, section, gpt_out])    \n",
    "        except Exception as err:\n",
    "            num_tries[(run_key,i)] += 1\n",
    "            print(f\"Encountered an exception for row: '{name}' '{section}'.\")\n",
    "            print(f\"This is the {num_tries[(run_key,i)]} time we have tried to run this. Will try {max_tries} times and then skip.\")\n",
    "            print(f\"Will save progress, so you can restart from where we left off.\")\n",
    "            gpt_output = pd.DataFrame(\n",
    "                [r for r in results if r is not None],\n",
    "                columns=['drug_name', 'section_name', 'gpt_output']\n",
    "            )\n",
    "            if gpt_output.shape[0] > 0:\n",
    "                print(\"Saved progress successfully.\")\n",
    "                outputs[run_key] = gpt_output\n",
    "                gpt_output.to_csv('results/extract/{}.csv'.format(run_key))\n",
    "            \n",
    "            print(f\"Failed for prompt: {prompt.format(text)}\")\n",
    "            raise err\n",
    "            continue\n",
    "    \n",
    "    gpt_output = pd.DataFrame(\n",
    "        [r for r in results if r is not None],\n",
    "        columns=['drug_name', 'section_name', 'gpt_output']\n",
    "    )\n",
    "    end = time.time()\n",
    "    \n",
    "    if gpt_output.shape[0] > 0:\n",
    "        outputs[run_key] = gpt_output\n",
    "        gpt_output.to_csv('results/extract/{}.csv'.format(run_key))\n",
    "    \n",
    "    print(f\"Run: {run_key}, time elapsed: {end-start}s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f659cb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7986085b",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "787d7804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running strict evaluation and saving results to disk.\n",
      "Mixtral-8x7B-Instruct-v0.1_gpt-written-prompt-mixtral_pharmexpert-v1_temp0_train_run0\n",
      "saving results to results/evals/Mixtral-8x7B-Instruct-v0.1_gpt-written-prompt-mixtral_pharmexpert-v1_temp0_train_run0_strict_granular.csv and results/evals/Mixtral-8x7B-Instruct-v0.1_gpt-written-prompt-mixtral_pharmexpert-v1_temp0_train_run0_strict_overall.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:02<00:00, 36.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeLlama-34b-Instruct-hf_gpt-written-prompt-llama_pharmexpert-v1_temp0_train_run0\n",
      "saving results to results/evals/CodeLlama-34b-Instruct-hf_gpt-written-prompt-llama_pharmexpert-v1_temp0_train_run0_strict_granular.csv and results/evals/CodeLlama-34b-Instruct-hf_gpt-written-prompt-llama_pharmexpert-v1_temp0_train_run0_strict_overall.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:02<00:00, 38.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running lenient evaluation and saving results to disk.\n",
      "Mixtral-8x7B-Instruct-v0.1_gpt-written-prompt-mixtral_pharmexpert-v1_temp0_train_run0\n",
      "saving results to results/evals/Mixtral-8x7B-Instruct-v0.1_gpt-written-prompt-mixtral_pharmexpert-v1_temp0_train_run0_lenient_granular.csv and results/evals/Mixtral-8x7B-Instruct-v0.1_gpt-written-prompt-mixtral_pharmexpert-v1_temp0_train_run0_lenient_overall.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:13<00:00,  7.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeLlama-34b-Instruct-hf_gpt-written-prompt-llama_pharmexpert-v1_temp0_train_run0\n",
      "saving results to results/evals/CodeLlama-34b-Instruct-hf_gpt-written-prompt-llama_pharmexpert-v1_temp0_train_run0_lenient_granular.csv and results/evals/CodeLlama-34b-Instruct-hf_gpt-written-prompt-llama_pharmexpert-v1_temp0_train_run0_lenient_overall.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:12<00:00,  8.34it/s]\n"
     ]
    }
   ],
   "source": [
    "evaluate(outputs, manual_ades, 'strict')\n",
    "evaluate(outputs, manual_ades, 'lenient')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b1744345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using embeddings -- run this once:\n",
    "# get embeddings for manual annotation --- this part is slow -- but should take <5 min\n",
    "embed_model_name = 'llmrails/ember-v1'\n",
    "embed_model = SentenceTransformer(embed_model_name)\n",
    "man_embeds = embed_model.encode(manual_ades['reaction_string'].tolist())\n",
    "manual_ades['embeds'] = list(man_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e6d06f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running embed evaluation and saving results to disk.\n",
      "Mixtral-8x7B-Instruct-v0.1_gpt-written-prompt-mixtral_pharmexpert-v1_temp0_train_run0\n",
      "saving results to results/evals/Mixtral-8x7B-Instruct-v0.1_gpt-written-prompt-mixtral_pharmexpert-v1_temp0_train_run0_ember-v1_granular.csv and results/evals/Mixtral-8x7B-Instruct-v0.1_gpt-written-prompt-mixtral_pharmexpert-v1_temp0_train_run0_ember-v1_overall.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [01:08<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeLlama-34b-Instruct-hf_gpt-written-prompt-llama_pharmexpert-v1_temp0_train_run0\n",
      "saving results to results/evals/CodeLlama-34b-Instruct-hf_gpt-written-prompt-llama_pharmexpert-v1_temp0_train_run0_ember-v1_granular.csv and results/evals/CodeLlama-34b-Instruct-hf_gpt-written-prompt-llama_pharmexpert-v1_temp0_train_run0_ember-v1_overall.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:54<00:00,  1.87it/s]\n"
     ]
    }
   ],
   "source": [
    "evaluate(outputs, manual_ades, 'embed', embed_model=embed_model, embed_model_name=embed_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a46975a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
