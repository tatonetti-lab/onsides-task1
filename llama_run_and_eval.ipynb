{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "706f8453-d366-4e8c-976c-90b59cf58197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "from evaluation_functions import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db80a89-6744-4602-936e-c5457d547d20",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8759fb31-5cee-46e8-a65f-36bf14b08730",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2ed0377f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_extraction(model, prompt, text, temperature, max_length):\n",
    "    # ssh -N -f -L localhost:5000:localhost:5000 username@10.19.2.120\n",
    "    llamasgard_endpoint = \"http://localhost:5000/predict\"\n",
    "\n",
    "    # model = codellama/CodeLlama-34b-Instruct-hf\n",
    "    # Define the payload\n",
    "    payload = {\n",
    "        \"input\": prompt.format(text),\n",
    "        \"model_id\": model,\n",
    "        \"parameters\": {\n",
    "            \"temperature\": temperature,\n",
    "            \"max_length\": max_length\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    #print(prompt.format(text))\n",
    "\n",
    "    response = requests.post(llamasgard_endpoint, json=payload)\n",
    "\n",
    "    # Check the response status code\n",
    "    if response.status_code != 200:\n",
    "        print(\"Error:\", response.json())\n",
    "\n",
    "    #print(\"\\n\\n\" + str(response.json()) + \"\\n\")\n",
    "    return response.json().get('response')\n",
    "\n",
    "def perform_cleanup(extraction, openai_api):\n",
    "    client = OpenAI(api_key=openai_api)\n",
    "    \n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"\"},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"\"\"The following text is an extraction of adverse event terms from a drug label. Please remove any preamble or postamble from the list and turn the list of ADEs into a comma separated list. \n",
    "The text: {}\"\"\".format(extraction)\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-3.5-turbo-16k\",\n",
    "        temperature=0,\n",
    "    )\n",
    "    term = chat_completion.choices[0].message.content\n",
    "    return term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "57ec29cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# row['section_text']\n",
    "# perform_extraction(\"google/gemma-7b\", system_content, prompt, row['section_text'], 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "98fa878b-b40a-4895-808e-9574b7d004c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for extracting \n",
    "def extract_ade_terms(config, model, prompt, text, temperature, max_length):\n",
    "  extraction = perform_extraction(model, prompt, text, temperature, max_length)\n",
    "  if extraction is None:\n",
    "    raise Exception(f\"perform_extraction() return None for {model}\")\n",
    "  else:\n",
    "    extraction = perform_cleanup(extraction, config['OpenAI']['openai_api_key'])\n",
    "    return extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37930311-07c7-4762-abd1-cb249c5bd25d",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ed8cc025-3af6-4bea-826b-962eb8b36f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_file = 'data/train_drug_label_text.csv'\n",
    "manual_file = 'data/train_drug_label_text_manual_ades.csv'\n",
    "my_max = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "22518e4d-ec19-4a56-9914-cd969a50cbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "drugs = pd.read_csv(drug_file)\n",
    "manual_ades = pd.read_csv(manual_file)\n",
    "set_type = drug_file.split('/')[1].split('_')[0] # assuming file follows format \"train_...\" or \"test....\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82ae7da-2a0e-4755-ae4d-4bced5f7f5a2",
   "metadata": {},
   "source": [
    "## Run Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e618b1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f6f04e8a-be72-4ad8-abfe-7b5d60c03a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = json.load(open('./config.json'))\n",
    "\n",
    "# gpt_model = 'code-llama-34b'\n",
    "# model_id = \"codellama/CodeLlama-34b-Instruct-hf\"\n",
    "model_id = \"google/gemma-7b\"\n",
    "\n",
    "model_name = model_id.split('/')[1]\n",
    "\n",
    "max_length = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b4b1bbd7-3b6c-42f5-a2af-04391ea86537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gemma-7b_fatal-prompt-v2_pharmexpert-v0_temp0_train'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nruns = 1\n",
    "temperature = 0\n",
    "\n",
    "system_options = {\n",
    "    \"no-system-prompt\": \"\",\n",
    "    \"pharmexpert-v0\": \"You are an expert in pharmacology.\",\n",
    "    \"pharmexpert-v1\": \"You are an expert in medical natural language processing, adverse drug reactions, pharmacology, and clinical trials.\"\n",
    "}\n",
    "\n",
    "prompt_options = {\n",
    "    \"fatal-prompt-v2\": \"\"\"\n",
    "Extract all adverse reactions as they appear, including all synonyms.\n",
    "mentioned in the text and provide them as a comma-separated list.\n",
    "If a fatal event is listed add 'death' to the list.\n",
    "The text is :'{}' \n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "system_name = \"pharmexpert-v0\"\n",
    "system_content = system_options[system_name]\n",
    "\n",
    "user_prompt_name = \"fatal-prompt-v2\"\n",
    "user_prompt = prompt_options[user_prompt_name]\n",
    "\n",
    "gpt_params = [f\"temp{temperature}\"]\n",
    "\n",
    "if model_id.startswith(\"codellama\"):\n",
    "    prefix = \"\"\n",
    "    prompt = f\"<s>[INST] <<SYS>>\\\\n{system_content}\\\\n<</SYS>>\\\\n\\\\n{user_prompt}[/INST]{prefix}\"\n",
    "else:\n",
    "    prompt = system_content + '\\n' + user_prompt\n",
    "\n",
    "output_file_basename = '{}_{}_{}_{}_{}'.format(model_name, prompt_name, system_name, '-'.join(gpt_params), set_type)\n",
    "output_file_basename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "492d7e82-894f-4c0f-bbf1-4de819b94b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemma-7b_fatal-prompt-v2_pharmexpert-v0_temp0_train_run0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/239 [00:04<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: {'error': 'CUDA out of memory. Tried to allocate 3.45 GiB (GPU 0; 47.54 GiB total capacity; 13.77 GiB already allocated; 2.37 GiB free; 15.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF'}\n",
      "Encountered an exception for row: KYPROLIS adverse reactions. Error message below:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "perform_extraction() return None for google/gemma-7b",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered an exception for row: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msection\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Error message below:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     32\u001b[0m gpt_output \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[1;32m     33\u001b[0m     [r \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results \u001b[38;5;28;01mif\u001b[39;00m r \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m],\n\u001b[1;32m     34\u001b[0m     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdrug_name\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msection_name\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt_output\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     35\u001b[0m )\n",
      "Cell \u001b[0;32mIn[85], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m text \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msection_text\u001b[39m\u001b[38;5;124m'\u001b[39m][:\u001b[38;5;241m15000\u001b[39m]\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m     gpt_out \u001b[38;5;241m=\u001b[39m \u001b[43mextract_ade_terms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend([name, section, gpt_out])\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "Cell \u001b[0;32mIn[66], line 5\u001b[0m, in \u001b[0;36mextract_ade_terms\u001b[0;34m(config, model, prompt, text, temperature, max_length)\u001b[0m\n\u001b[1;32m      3\u001b[0m extraction \u001b[38;5;241m=\u001b[39m perform_extraction(model, prompt, text, temperature, max_length)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extraction \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m----> 5\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperform_extraction() return None for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m   extraction \u001b[38;5;241m=\u001b[39m perform_cleanup(extraction, config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOpenAI\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenai_api_key\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mException\u001b[0m: perform_extraction() return None for google/gemma-7b"
     ]
    }
   ],
   "source": [
    "# run Llama\n",
    "for i in range(nruns):\n",
    "    run_key = \"{}_run{}\".format(output_file_basename, i)\n",
    "    print(run_key)\n",
    "    if run_key in outputs:\n",
    "        print(f\"Run {run_key} already started will pick up from where it was left off.\")\n",
    "    elif os.path.exists('results/{}.csv'.format(run_key)):\n",
    "        gpt_output = pd.read_csv('results/{}.csv'.format(run_key))\n",
    "        outputs[run_key] = gpt_output\n",
    "        print(f\"Run {run_key} started, loading from disk and pick up from where it was left off.\")\n",
    "    \n",
    "    start = time.time()\n",
    "    results = list()\n",
    "    for _, row in tqdm(drugs.iterrows(), total=drugs.shape[0]):\n",
    "        name, section = row['drug_name'], row['section_name']\n",
    "\n",
    "        if run_key in outputs:\n",
    "            prev_run_results = outputs[run_key].query(f\"drug_name == '{name}'\").query(f\"section_name == '{section}'\")\n",
    "            if prev_run_results.shape[0]==1:\n",
    "                results.append([name, section, prev_run_results.gpt_output.values[0]])\n",
    "                continue\n",
    "        \n",
    "        text = row['section_text'][:15000]\n",
    "        try:\n",
    "            gpt_out = extract_ade_terms(config, model_id, prompt, text, temperature, max_length)\n",
    "            results.append([name, section, gpt_out])\n",
    "        except Exception as err:\n",
    "            print(f\"Encountered an exception for row: {name} {section}. Error message below:\")\n",
    "            raise err\n",
    "            continue\n",
    "    \n",
    "    gpt_output = pd.DataFrame(\n",
    "        [r for r in results if r is not None],\n",
    "        columns=['drug_name', 'section_name', 'gpt_output']\n",
    "    )\n",
    "    end = time.time()\n",
    "    \n",
    "    if gpt_output.shape[0] > 0:\n",
    "        outputs[run_key] = gpt_output\n",
    "        gpt_output.to_csv('results/{}.csv'.format(run_key))\n",
    "    \n",
    "    print(f\"Run: {run_key}, time elapsed: {end-start}s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7986085b",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "787d7804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running strict evaluation and saving results to disk.\n",
      "code-llama-34b_fatal-prompt-v2_pharmexpert-v0_temp0_train_run0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:02<00:00, 50.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running lenient evaluation and saving results to disk.\n",
      "code-llama-34b_fatal-prompt-v2_pharmexpert-v0_temp0_train_run0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:15<00:00,  6.50it/s]\n"
     ]
    }
   ],
   "source": [
    "evaluate(outputs, manual_ades, 'strict')\n",
    "evaluate(outputs, manual_ades, 'lenient')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1744345",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
