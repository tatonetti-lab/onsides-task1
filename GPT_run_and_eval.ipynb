{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "706f8453-d366-4e8c-976c-90b59cf58197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from openai import OpenAI, AzureOpenAI\n",
    "import csv\n",
    "import numpy as np\n",
    "import concurrent\n",
    "import time\n",
    "import json\n",
    "from common_string import common_lenient_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db80a89-6744-4602-936e-c5457d547d20",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8759fb31-5cee-46e8-a65f-36bf14b08730",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98fa878b-b40a-4895-808e-9574b7d004c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for running GPT\n",
    "def extract_ade_terms(api_source, api_endpoint, gpt_model, system_content, prompt, text, openai_api):\n",
    "  if api_source == 'OpenAI':\n",
    "    client = OpenAI(api_key=openai_api)\n",
    "  elif api_source == 'Azure':\n",
    "    client = AzureOpenAI(api_key=openai_api, api_version=\"2023-12-01-preview\", azure_endpoint=api_endpoint)\n",
    "  else:\n",
    "    raise Exception(f\"Unexpected API source requested: {api_source}\")\n",
    "  \n",
    "  chat_completion = client.chat.completions.create(\n",
    "      messages=[\n",
    "          {\"role\": \"system\", \"content\": system_content},\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": prompt.format(text)\n",
    "          }\n",
    "      ],\n",
    "      model=gpt_model\n",
    "  )\n",
    "  term = chat_completion.choices[0].message.content\n",
    "  return term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd0bffb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_subtype(manual_ades, gpt_output, drug, subtype = 'all', lenient=False):\n",
    "    '''\n",
    "    For a given drug, evaluate the performance of GPT on a given subtype of ADEs. \n",
    "    '''\n",
    "    \n",
    "    drug_df = manual_ades.query(\"(drug_name == '{}') & (section_name == 'adverse reactions')\".format(drug))\n",
    "    if subtype == 'exact-meddra': drug_df = drug_df[drug_df.meddra_exact_term == 1]\n",
    "    if subtype == 'non-meddra': drug_df = drug_df[drug_df.meddra_exact_term == 0]\n",
    "    if subtype == 'negated': drug_df = drug_df[drug_df.negated_term == 1]\n",
    "    if subtype == 'discontinuous': drug_df = drug_df[drug_df.discontinuous_term == 1]\n",
    "\n",
    "    \n",
    "    manual = set(drug_df['reaction_string'].to_list())\n",
    "    gpt_drug = (gpt_output[\n",
    "        (gpt_output['drug_name'] == drug)\n",
    "        &\n",
    "        (gpt_output['section_name'] == \"adverse reactions\")\n",
    "        ][\"gpt_output\"].astype(str)\n",
    "        .str.lower()\n",
    "        .str.replace('\\n-', ', ')\n",
    "        .str.split(\",\").tolist())\n",
    "\n",
    "    try:\n",
    "        gpt_drug = [x.strip() for x in gpt_drug[0]]\n",
    "        gpt_drug = set(gpt_drug)\n",
    "    except:\n",
    "        return [drug, subtype, len(manual), len(gpt_drug), np.nan, np.nan, np.nan, np.nan, np.nan, np.nan]\n",
    "        \n",
    "    if not lenient:    \n",
    "        #overall\n",
    "        TP = len(manual.intersection(gpt_drug))\n",
    "        FP = len(gpt_drug.difference(manual))\n",
    "        FN = len(manual.difference(gpt_drug))\n",
    "        if TP == 0 and FP == 0:\n",
    "            precision = np.NAN\n",
    "        else:\n",
    "            precision = TP/(TP+FP)\n",
    "        if TP == 0 and FN == 0:\n",
    "            recall = np.NAN\n",
    "        else:\n",
    "            recall = TP/(TP+FN)\n",
    "        if precision != 0 and recall != 0:\n",
    "            f1 = (2 * precision * recall)/(precision + recall)# 2*TP/(2*TP+FP+FN)\n",
    "        else:\n",
    "            f1 = np.NAN\n",
    "    else:\n",
    "        [TP, FP, FN, precision, recall, f1] = common_lenient_performance(gpt_drug, manual)\n",
    "    \n",
    "    if subtype != 'all':\n",
    "            # these can't be computed for the subtypes\n",
    "            precision = np.nan\n",
    "            f1 = np.nan\n",
    "            FP = np.nan\n",
    "    \n",
    "    return [drug, subtype, len(manual), len(gpt_drug), TP, FP, FN, precision, recall, f1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8c6cc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(manual_ades, gpt_output, lenient=False, limit = 1000):\n",
    "    drugs = gpt_output['drug_name'].unique()\n",
    "    drugs_set = set()\n",
    "    results = []\n",
    "    for drug in tqdm(drugs):\n",
    "            results.append(evaluation_subtype(manual_ades, gpt_output, drug, lenient))        \n",
    "    results = pd.DataFrame(results, columns=['drug_name', 'exclude', 'n_manual', 'n_gpt', 'tp', 'fp', 'fn', 'precision', 'recall', 'f1'])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d669d0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_granular(manual_ades, gpt_output, limit = 1000, lenient=False):\n",
    "    drugs = gpt_output['drug_name'].unique()\n",
    "    drugs_set = set()\n",
    "    results = []\n",
    "    for drug in tqdm(drugs):\n",
    "        drugs_set.add(drug)\n",
    "        if len(drugs_set) > limit:\n",
    "            break\n",
    "        \n",
    "        results.append(evaluation_subtype(manual_ades, gpt_output, drug, subtype = 'all', lenient=lenient))\n",
    "        results.append(evaluation_subtype(manual_ades, gpt_output, drug, subtype = 'exact-meddra',lenient=lenient))\n",
    "        results.append(evaluation_subtype(manual_ades, gpt_output, drug, subtype = 'non-meddra',lenient=lenient))\n",
    "        results.append(evaluation_subtype(manual_ades, gpt_output, drug, subtype = 'negated',lenient=lenient))\n",
    "        results.append(evaluation_subtype(manual_ades, gpt_output, drug, subtype = 'discontinuous',lenient=lenient))\n",
    "\n",
    "    results = pd.DataFrame(results, columns=['drug_name', 'ade_type', 'n_manual', 'n_gpt', 'tp', 'fp', 'fn', 'precision', 'recall', 'f1'])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37930311-07c7-4762-abd1-cb249c5bd25d",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed8cc025-3af6-4bea-826b-962eb8b36f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_file = 'data/train_drug_label_text.csv'\n",
    "manual_file = 'data/train_drug_label_text_manual_ades.csv'\n",
    "my_max = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22518e4d-ec19-4a56-9914-cd969a50cbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "drugs = pd.read_csv(drug_file)\n",
    "manual_ades = pd.read_csv(manual_file)\n",
    "set_type = drug_file.split('/')[1].split('_')[0] # assuming file follows format \"train_...\" or \"test....\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82ae7da-2a0e-4755-ae4d-4bced5f7f5a2",
   "metadata": {},
   "source": [
    "## Run GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e618b1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6f04e8a-be72-4ad8-abfe-7b5d60c03a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = json.load(open('./config.json'))\n",
    "\n",
    "organization = \"\"\n",
    "\n",
    "api_source = 'OpenAI'\n",
    "\n",
    "api_key = config[api_source]['openai_api_key'] #constants.AZURE_OPENAI_KEY\n",
    "api_endpoint = config[api_source]['openai_api_endpoint'] \n",
    "\n",
    "gpt_model = config[api_source][\"gpt_model\"]\n",
    "# gpt_model = \"gpt-4-turbo-preview\"\n",
    "# gpt_model = \"gpt-3.5-turbo-0125\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4b1bbd7-3b6c-42f5-a2af-04391ea86537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OpenAI_gpt-4-1106-preview_fatal-prompt-v2_pharmexpert-v1_train'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nruns = 15\n",
    "\n",
    "system_options = {\n",
    "    \"pharmexpert-v0\": \"You are an expert in pharmacology.\",\n",
    "    \"pharmexpert-v1\": \"You are an expert in medical natural language processing, adverse drug reactions, pharmacology, and clinical trials.\"\n",
    "}\n",
    "\n",
    "prompt_options = {\n",
    "    \"fatal-prompt-v2\": \"\"\"\n",
    "Extract all adverse reactions as they appear, including all synonyms.\n",
    "mentioned in the text and provide them as a comma-separated list.\n",
    "If a fatal event is listed add 'death' to the list.\n",
    "The text is :'{}' \n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "system_name = \"pharmexpert-v1\"\n",
    "system_content = system_options[system_name]\n",
    "\n",
    "prompt_name = \"fatal-prompt-v2\"\n",
    "prompt = prompt_options[prompt_name]\n",
    "\n",
    "output_file_basename = '{}_{}_{}_{}_{}'.format(api_source, gpt_model, prompt_name, system_name, set_type)\n",
    "output_file_basename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdb9c192-ba44-4135-8d3c-7b6ba6644563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if there is a max\n",
    "new_rows = list()\n",
    "unique_drugs = set()\n",
    "for i, row in drugs.iterrows():\n",
    "    unique_drugs.add(row[\"drug_name\"])\n",
    "    if len(unique_drugs) > my_max: \n",
    "        break\n",
    "    if row['section_name'] != 'adverse reactions':\n",
    "        continue\n",
    "\n",
    "    new_rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "492d7e82-894f-4c0f-bbf1-4de819b94b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run OpenAI_gpt-4-1106-preview_fatal-prompt-v2_pharmexpert-v1_train_run0 already completed and loaded from disk.\n",
      "Run OpenAI_gpt-4-1106-preview_fatal-prompt-v2_pharmexpert-v1_train_run1 already completed and loaded from disk.\n",
      "Run OpenAI_gpt-4-1106-preview_fatal-prompt-v2_pharmexpert-v1_train_run2 already completed and loaded from disk.\n",
      "Run OpenAI_gpt-4-1106-preview_fatal-prompt-v2_pharmexpert-v1_train_run3 already completed and loaded from disk.\n",
      "Run OpenAI_gpt-4-1106-preview_fatal-prompt-v2_pharmexpert-v1_train_run4 already completed and loaded from disk.\n",
      "Run OpenAI_gpt-4-1106-preview_fatal-prompt-v2_pharmexpert-v1_train_run5 already completed and loaded from disk.\n",
      "Run OpenAI_gpt-4-1106-preview_fatal-prompt-v2_pharmexpert-v1_train_run6 already completed and loaded from disk.\n",
      "Run OpenAI_gpt-4-1106-preview_fatal-prompt-v2_pharmexpert-v1_train_run7 already completed and loaded from disk.\n",
      "Run OpenAI_gpt-4-1106-preview_fatal-prompt-v2_pharmexpert-v1_train_run8 already completed and loaded from disk.\n",
      "Run OpenAI_gpt-4-1106-preview_fatal-prompt-v2_pharmexpert-v1_train_run9 already completed and loaded from disk.\n",
      "Run OpenAI_gpt-4-1106-preview_fatal-prompt-v2_pharmexpert-v1_train_run10 already completed and loaded from disk.\n",
      "Run OpenAI_gpt-4-1106-preview_fatal-prompt-v2_pharmexpert-v1_train_run11 already completed and loaded from disk.\n",
      "Run OpenAI_gpt-4-1106-preview_fatal-prompt-v2_pharmexpert-v1_train_run12 already completed and loaded from disk.\n",
      "Run OpenAI_gpt-4-1106-preview_fatal-prompt-v2_pharmexpert-v1_train_run13 already completed and loaded from disk.\n",
      "Run OpenAI_gpt-4-1106-preview_fatal-prompt-v2_pharmexpert-v1_train_run14 already completed and loaded from disk.\n"
     ]
    }
   ],
   "source": [
    "# run GPT\n",
    "for i in range(nruns):\n",
    "    run_key = \"{}_run{}\".format(output_file_basename, i)\n",
    "\n",
    "    if run_key in outputs:\n",
    "        print(f\"Run {run_key} already completed and stored. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    if os.path.exists('results/{}.csv'.format(run_key)):\n",
    "        gpt_output = pd.read_csv('results/{}.csv'.format(run_key))\n",
    "        outputs[run_key] = gpt_output\n",
    "        print(f\"Run {run_key} already completed and loaded from disk.\")\n",
    "        continue\n",
    "    \n",
    "    start = time.time()\n",
    "    results = list()\n",
    "    for row in tqdm(new_rows):\n",
    "        name, section = row['drug_name'], row['section_name']\n",
    "        text = row['section_text']\n",
    "        try:\n",
    "            gpt_out = extract_ade_terms(api_source, api_endpoint, gpt_model, system_content, prompt, text, api_key)\n",
    "            # time.sleep(5)\n",
    "            results.append([name, section, gpt_out])\n",
    "        except Exception as err:\n",
    "            print(f\"Encountered an exception for row: {name} {section}. Error message below:\")\n",
    "            print(err)\n",
    "            continue\n",
    "            \n",
    "    gpt_output = pd.DataFrame(\n",
    "        [r for r in results if r is not None],\n",
    "        columns=['drug_name', 'section_name', 'gpt_output']\n",
    "    )\n",
    "    end = time.time()\n",
    "\n",
    "    if gpt_output.shape[0] > 0:\n",
    "        outputs[run_key] = gpt_output\n",
    "        gpt_output.to_csv('results/{}.csv'.format(run_key))\n",
    "    \n",
    "    print(f\"Run: {run_key}, time elapsed: {end-start}s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "490cab7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['OpenAI_gpt-4-1106-preview_fatal-prompt-v2_pharmexpert-v1_train_run0', 'OpenAI_gpt-4-1106-preview_fatal-prompt-v2_pharmexpert-v1_train_run1', 'OpenAI_gpt-4-1106-preview_fatal-prompt-v2_pharmexpert-v1_train_run2', 'OpenAI_gpt-4-1106-preview_fatal-prompt-v2_pharmexpert-v1_train_run3', 'OpenAI_gpt-4-1106-preview_fatal-prompt-v2_pharmexpert-v1_train_run4', 'OpenAI_gpt-4-1106-preview_fatal-prompt-v2_pharmexpert-v1_train_run5', 'OpenAI_gpt-4-1106-preview_fatal-prompt-v2_pharmexpert-v1_train_run6', 'OpenAI_gpt-4-1106-preview_fatal-prompt-v2_pharmexpert-v1_train_run7', 'OpenAI_gpt-4-1106-preview_fatal-prompt-v2_pharmexpert-v1_train_run8', 'OpenAI_gpt-4-1106-preview_fatal-prompt-v2_pharmexpert-v1_train_run9', 'OpenAI_gpt-4-1106-preview_fatal-prompt-v2_pharmexpert-v1_train_run10', 'OpenAI_gpt-4-1106-preview_fatal-prompt-v2_pharmexpert-v1_train_run11', 'OpenAI_gpt-4-1106-preview_fatal-prompt-v2_pharmexpert-v1_train_run12', 'OpenAI_gpt-4-1106-preview_fatal-prompt-v2_pharmexpert-v1_train_run13', 'OpenAI_gpt-4-1106-preview_fatal-prompt-v2_pharmexpert-v1_train_run14'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79125bf3",
   "metadata": {},
   "source": [
    "## Exact Match Algorithm\n",
    "\n",
    "TODO: Exact match really doesn't fit here any longer but we need it at least once to show how well (or poorly) it does. Refactor out into it's own notebook/script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "704c226e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_key = 'exact_{}'.format(set_type)\n",
    "\n",
    "# if not os.path.exists('results/{}.csv'.format(run_key)):\n",
    "#     # load the meddra terms\n",
    "#     fh = open('data/meddra_llt_pt_map.txt')\n",
    "#     reader = csv.reader(fh, delimiter='|')\n",
    "#     header = next(reader)\n",
    "\n",
    "#     meddra_llt_terms = set()\n",
    "#     meddra_pt_terms = set()\n",
    "\n",
    "#     for row in reader:\n",
    "#         meddra_llt_terms.add(row[1].lower())\n",
    "#         meddra_pt_terms.add(row[4].lower())\n",
    "    \n",
    "#     fh.close()\n",
    "\n",
    "#     meddra_terms = meddra_llt_terms | meddra_pt_terms\n",
    "#     len(meddra_llt_terms), len(meddra_pt_terms), len(meddra_terms)\n",
    "\n",
    "#     results = list()\n",
    "#     for row in tqdm(new_rows):\n",
    "#         name, section = row['drug_name'], row['section_name']\n",
    "#         text = row['section_text'].lower()\n",
    "        \n",
    "#         found_terms = set()\n",
    "#         for term in meddra_terms:\n",
    "#             if text.find(term) != -1:\n",
    "#                 found_terms.add(term)\n",
    "        \n",
    "#         exact_out = ', '.join(list(found_terms))\n",
    "        \n",
    "#         results.append([name, section, exact_out])\n",
    "\n",
    "#     exact_output = pd.DataFrame(\n",
    "#         [r for r in results if r is not None],\n",
    "#         columns=['drug_name', 'section_name', 'gpt_output']\n",
    "#     )\n",
    "#     exact_output.to_csv('results/{}.csv'.format(run_key))\n",
    "    \n",
    "#     outputs[run_key] = exact_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7986085b",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "787d7804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/OpenAI_gpt-4-1106-preview_fatal-prompt-v2_pharmexpert-v1_train_run0_strict_granular.csv\n",
      "results/OpenAI_gpt-4-1106-preview_fatal-prompt-v2_pharmexpert-v1_train_run1_strict_granular.csv\n",
      "results/OpenAI_gpt-4-1106-preview_fatal-prompt-v2_pharmexpert-v1_train_run2_strict_granular.csv\n",
      "results/OpenAI_gpt-4-1106-preview_fatal-prompt-v2_pharmexpert-v1_train_run3_strict_granular.csv\n",
      "results/OpenAI_gpt-4-1106-preview_fatal-prompt-v2_pharmexpert-v1_train_run4_strict_granular.csv\n",
      "results/OpenAI_gpt-4-1106-preview_fatal-prompt-v2_pharmexpert-v1_train_run5_strict_granular.csv\n",
      "results/OpenAI_gpt-4-1106-preview_fatal-prompt-v2_pharmexpert-v1_train_run6_strict_granular.csv\n",
      "results/OpenAI_gpt-4-1106-preview_fatal-prompt-v2_pharmexpert-v1_train_run7_strict_granular.csv\n",
      "results/OpenAI_gpt-4-1106-preview_fatal-prompt-v2_pharmexpert-v1_train_run8_strict_granular.csv\n",
      "results/OpenAI_gpt-4-1106-preview_fatal-prompt-v2_pharmexpert-v1_train_run9_strict_granular.csv\n",
      "results/OpenAI_gpt-4-1106-preview_fatal-prompt-v2_pharmexpert-v1_train_run10_strict_granular.csv\n",
      "results/OpenAI_gpt-4-1106-preview_fatal-prompt-v2_pharmexpert-v1_train_run11_strict_granular.csv\n",
      "results/OpenAI_gpt-4-1106-preview_fatal-prompt-v2_pharmexpert-v1_train_run12_strict_granular.csv\n",
      "results/OpenAI_gpt-4-1106-preview_fatal-prompt-v2_pharmexpert-v1_train_run13_strict_granular.csv\n",
      "results/OpenAI_gpt-4-1106-preview_fatal-prompt-v2_pharmexpert-v1_train_run14_strict_granular.csv\n"
     ]
    }
   ],
   "source": [
    "for run_key, output in outputs.items():\n",
    "    save_filename = 'results/{}_strict_granular.csv'.format(run_key)\n",
    "    print(save_filename)\n",
    "    if os.path.exists(save_filename):\n",
    "        continue\n",
    "    \n",
    "    results_granular = evaluation_granular(manual_ades, output)\n",
    "    overall_results = results_granular.groupby('ade_type')[['tp', 'fp', 'fn']].sum(min_count = 1).reset_index()\n",
    "    overall_results['micro_precision'] = overall_results['tp']/(overall_results['tp']+overall_results['fp'])\n",
    "    overall_results['micro_recall'] = overall_results['tp']/(overall_results['tp']+overall_results['fn'])\n",
    "    overall_results['micro_f1'] = (2 * overall_results['micro_precision'] * overall_results['micro_recall'])/(overall_results['micro_precision'] + overall_results['micro_recall']) # 2*tp_total/(2*tp_total+fp_total+fn_total)\n",
    "    macro_results = results_granular.groupby('ade_type')[['precision', 'recall', 'f1']].mean(numeric_only=True).reset_index()\n",
    "    overall_results['macro_precision'] = macro_results['precision']\n",
    "    overall_results['macro_recall'] = macro_results['recall']\n",
    "    overall_results['macro_f1'] = macro_results['f1']\n",
    "    \n",
    "    overall_results.to_csv(save_filename)\n",
    "    print(run_key)\n",
    "    print(overall_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346b793e",
   "metadata": {},
   "source": [
    "#### Lenient Matching using Longest Commmon Substring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78dd1292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/OpenAI_gpt-4-1106-preview_fatal-prompt-v2_pharmexpert-v1_train_run0_lenient_granular.csv\n",
      "results/OpenAI_gpt-4-1106-preview_fatal-prompt-v2_pharmexpert-v1_train_run1_lenient_granular.csv\n",
      "results/OpenAI_gpt-4-1106-preview_fatal-prompt-v2_pharmexpert-v1_train_run2_lenient_granular.csv\n",
      "results/OpenAI_gpt-4-1106-preview_fatal-prompt-v2_pharmexpert-v1_train_run3_lenient_granular.csv\n",
      "results/OpenAI_gpt-4-1106-preview_fatal-prompt-v2_pharmexpert-v1_train_run4_lenient_granular.csv\n",
      "results/OpenAI_gpt-4-1106-preview_fatal-prompt-v2_pharmexpert-v1_train_run5_lenient_granular.csv\n",
      "results/OpenAI_gpt-4-1106-preview_fatal-prompt-v2_pharmexpert-v1_train_run6_lenient_granular.csv\n",
      "results/OpenAI_gpt-4-1106-preview_fatal-prompt-v2_pharmexpert-v1_train_run7_lenient_granular.csv\n",
      "results/OpenAI_gpt-4-1106-preview_fatal-prompt-v2_pharmexpert-v1_train_run8_lenient_granular.csv\n",
      "results/OpenAI_gpt-4-1106-preview_fatal-prompt-v2_pharmexpert-v1_train_run9_lenient_granular.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/101 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 60/101 [00:04<00:03, 12.77it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "One of the given string is empty",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(save_filename):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m results_granular \u001b[38;5;241m=\u001b[39m \u001b[43mevaluation_granular\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmanual_ades\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlenient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m overall_results \u001b[38;5;241m=\u001b[39m results_granular\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124made_type\u001b[39m\u001b[38;5;124m'\u001b[39m)[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfn\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39msum(min_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[1;32m     10\u001b[0m overall_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmicro_precision\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m overall_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtp\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m/\u001b[39m(overall_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtp\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m+\u001b[39moverall_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfp\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[5], line 10\u001b[0m, in \u001b[0;36mevaluation_granular\u001b[0;34m(manual_ades, gpt_output, limit, lenient)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(drugs_set) \u001b[38;5;241m>\u001b[39m limit:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m results\u001b[38;5;241m.\u001b[39mappend(\u001b[43mevaluation_subtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmanual_ades\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpt_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrug\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mall\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlenient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlenient\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     11\u001b[0m results\u001b[38;5;241m.\u001b[39mappend(evaluation_subtype(manual_ades, gpt_output, drug, subtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexact-meddra\u001b[39m\u001b[38;5;124m'\u001b[39m,lenient\u001b[38;5;241m=\u001b[39mlenient))\n\u001b[1;32m     12\u001b[0m results\u001b[38;5;241m.\u001b[39mappend(evaluation_subtype(manual_ades, gpt_output, drug, subtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnon-meddra\u001b[39m\u001b[38;5;124m'\u001b[39m,lenient\u001b[38;5;241m=\u001b[39mlenient))\n",
      "Cell \u001b[0;32mIn[3], line 47\u001b[0m, in \u001b[0;36mevaluation_subtype\u001b[0;34m(manual_ades, gpt_output, drug, subtype, lenient)\u001b[0m\n\u001b[1;32m     45\u001b[0m         f1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mNAN\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 47\u001b[0m     [TP, FP, FN, precision, recall, f1] \u001b[38;5;241m=\u001b[39m \u001b[43mcommon_lenient_performance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgpt_drug\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmanual\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m subtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;66;03m# these can't be computed for the subtypes\u001b[39;00m\n\u001b[1;32m     51\u001b[0m         precision \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnan\n",
      "File \u001b[0;32m~/Projects/onsides-task1/common_string.py:44\u001b[0m, in \u001b[0;36mcommon_lenient_performance\u001b[0;34m(gpt_output, manual_output)\u001b[0m\n\u001b[1;32m     41\u001b[0m         FP \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m man_out \u001b[38;5;129;01min\u001b[39;00m manual_output:\n\u001b[0;32m---> 44\u001b[0m     common \u001b[38;5;241m=\u001b[39m [longest_common_substring_percentage(man_out, x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m gpt_output]\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m([x \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m common]):\n\u001b[1;32m     46\u001b[0m         FN \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Projects/onsides-task1/common_string.py:44\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     41\u001b[0m         FP \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m man_out \u001b[38;5;129;01min\u001b[39;00m manual_output:\n\u001b[0;32m---> 44\u001b[0m     common \u001b[38;5;241m=\u001b[39m [\u001b[43mlongest_common_substring_percentage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mman_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m gpt_output]\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m([x \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m common]):\n\u001b[1;32m     46\u001b[0m         FN \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Projects/onsides-task1/common_string.py:18\u001b[0m, in \u001b[0;36mlongest_common_substring_percentage\u001b[0;34m(s1, s2)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlongest_common_substring_percentage\u001b[39m(s1 : \u001b[38;5;28mstr\u001b[39m, s2 : \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m     17\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Computes the longest common substring percentage of s1 and s2\"\"\"\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mlen\u001b[39m(s1), \u001b[38;5;28mlen\u001b[39m(s2)) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOne of the given string is empty\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(longest_common_substring(s1, s2))\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mlen\u001b[39m(s1), \u001b[38;5;28mlen\u001b[39m(s2))\n",
      "\u001b[0;31mAssertionError\u001b[0m: One of the given string is empty"
     ]
    }
   ],
   "source": [
    "# using lenient matching method 'longest common substring'\n",
    "for run_key, output in outputs.items():\n",
    "    save_filename = 'results/{}_lenient_granular.csv'.format(run_key)\n",
    "    print(save_filename)\n",
    "    if os.path.exists(save_filename):\n",
    "        continue\n",
    "    \n",
    "    results_granular = evaluation_granular(manual_ades, output, lenient=True)\n",
    "    overall_results = results_granular.groupby('ade_type')[['tp', 'fp', 'fn']].sum(min_count = 1).reset_index()\n",
    "    overall_results['micro_precision'] = overall_results['tp']/(overall_results['tp']+overall_results['fp'])\n",
    "    overall_results['micro_recall'] = overall_results['tp']/(overall_results['tp']+overall_results['fn'])\n",
    "    overall_results['micro_f1'] = (2 * overall_results['micro_precision'] * overall_results['micro_recall'])/(overall_results['micro_precision'] + overall_results['micro_recall']) # 2*tp_total/(2*tp_total+fp_total+fn_total)\n",
    "    macro_results = results_granular.groupby('ade_type')[['precision', 'recall', 'f1']].mean(numeric_only=True).reset_index()\n",
    "    overall_results['macro_precision'] = macro_results['precision']\n",
    "    overall_results['macro_recall'] = macro_results['recall']\n",
    "    overall_results['macro_f1'] = macro_results['f1']\n",
    "    \n",
    "    overall_results.to_csv(save_filename)\n",
    "    print(method)\n",
    "    print(overall_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d06d4cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
